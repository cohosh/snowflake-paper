#!/usr/bin/env python3

# Parses bridge-extra-info descriptors, filters them by WANTED_FINGERPRINTS, and
# writes a CSV file in the same format as those made by
# https://metrics.torproject.org/userstats-bridge-transport.html.
#
# This program differs from Tor Metrics in that it considers descriptors with
# the same identity fingerprint but a different nickname to be distinct.
#
# Usage:
#   userstats-bridge-transport bridge-extra-infos-*.tar.xz > userstats-bridge-transport.csv

import datetime
import getopt
import multiprocessing
import sys

import stem
import stem.descriptor
import stem.descriptor.reader
import stem.descriptor.networkstatus
import stem.descriptor.extrainfo_descriptor

import pandas as pd

def usage(f = sys.stdout):
    print(f"""\
Usage:
  {sys.argv[0]} [OPTION...] bridge-extra-infos-*.tar.xz > userstats-bridge-transport.csv

Options:
  -h, --help            show this help
  -j JOBS, --jobs=JOBS  run JOBS parsing jobs in parallel ({Options.num_jobs})\
""", file = f)

# Early version of this program:
# https://bugs.torproject.org/tpo/network-health/metrics/website/40047#note_2796619

# Reference for user estimation algorithms:
# https://metrics.torproject.org/reproducible-metrics.html#bridge-users

# "If the contained statistics end time is more than 1 week older than the
# descriptor publication time in the "published" line, skip this line..."
END_THRESHOLD = datetime.timedelta(days = 7)

# "Also skip statistics with an interval length other than 1 day."
# We set the threshold higher, because some descriptors have an interval a few
# seconds larger than 86400.
INTERVAL_THRESHOLD = datetime.timedelta(seconds = 90000)

# These are the hashed relay fingerprints we care about.
WANTED_FINGERPRINTS = set([
    "5481936581E23D2D178105D44DB6915AB06BFB7F", # flakey
    "91DA221A149007D0FD9E5515F5786C3DD07E4BB0", # crusty https://bugs.torproject.org/tpo/anti-censorship/pluggable-transports/snowflake/40122
])

class Options:
    num_jobs = 4 # Override with -j, --jobs option.
    verbose = True

def datetime_floor(d):
    return d.replace(hour = 0, minute = 0, second = 0, microsecond = 0)

TIMEDELTA_1DAY = datetime.timedelta(seconds = 86400)
def segment_datetime_interval(begin, end):
    cur = begin
    while cur < end:
        next = min(datetime_floor(cur + TIMEDELTA_1DAY), end)
        delta = next - cur
        yield (cur.date(), delta / (end - begin), delta / TIMEDELTA_1DAY)
        cur = next

def process_bridge_extra_infos(reader):
    bridge_transport_reqs = {
        "published": [],
        "fingerprint": [],
        "nickname": [],
        "begin": [],
        "end": [],
        "transport": [],
        "transport_reqs": [],
    }
    for desc in reader:
        assert type(desc) == stem.descriptor.extrainfo_descriptor.BridgeExtraInfoDescriptor, type(desc)

        if desc.fingerprint not in WANTED_FINGERPRINTS:
            continue

        if desc.dir_stats_end is not None \
            and desc.published - desc.dir_stats_end < END_THRESHOLD \
            and datetime.timedelta(seconds = desc.dir_stats_interval) < INTERVAL_THRESHOLD:
            # "Parse successful requests from the 'ok' part of the
            # 'dirreq-v3-resp' line, subtract 4 to undo the binning operation
            # that has been applied by the relay, and discard the resulting
            # number if it's zero or negative."
            resp_ok = max(0, desc.dir_v3_responses[stem.descriptor.extrainfo_descriptor.DirResponse.OK] - 4)

            if desc.bridge_stats_end is not None \
                and desc.published - desc.bridge_stats_end < END_THRESHOLD \
                and datetime.timedelta(seconds = desc.bridge_stats_interval) < INTERVAL_THRESHOLD:
                # "Parse the 'bridge-ips', 'bridge-ip-versions', and
                # 'bridge-ip-transports' lines containing unique connecting IP
                # addresses by country, IP version, and transport. From each
                # number of unique IP addresses, subtract 4 to undo the binning
                # operation that has been applied by the bridge. Discard the
                # resulting number if it's zero or negative."
                total_ips_transport = sum(max(0, ips - 4) for ips in desc.ip_transports.values())
                for transport, ips in desc.ip_transports.items():
                    ips = max(0, ips - 4)
                    bridge_transport_reqs["published"].append(desc.published)
                    bridge_transport_reqs["fingerprint"].append(desc.fingerprint)
                    bridge_transport_reqs["nickname"].append(desc.nickname)
                    bridge_transport_reqs["begin"].append(desc.bridge_stats_end - datetime.timedelta(seconds = desc.bridge_stats_interval))
                    bridge_transport_reqs["end"].append(desc.bridge_stats_end)
                    bridge_transport_reqs["transport"].append(transport)
                    # "Bridges do not report directory requests by transport or IP
                    # version. We approximate these numbers by multiplying the total
                    # number of requests with the fraction of unique IP addresses by
                    # transport or IP version."
                    bridge_transport_reqs["transport_reqs"].append(ips / total_ips_transport * resp_ok)

    # Keep only the most recent "published" for each "end", per bridge instance
    # ("fingerprint"+"nickname") and "transport".
    bridge_transport_reqs = (
        pd.DataFrame(bridge_transport_reqs)
            .sort_values("published")
            .groupby(["fingerprint", "nickname", "transport", "end"])
            .last()
            .reset_index()
    )

    return bridge_transport_reqs

def process_file(f):
    if Options.verbose:
        print(f, file = sys.stderr)
    with stem.descriptor.reader.DescriptorReader([f]) as reader:
        return process_bridge_extra_infos(reader)

if __name__ == "__main__":
    opts, inputs = getopt.gnu_getopt(sys.argv[1:], "hj:", [
        "help",
        "jobs=",
    ])
    for o, a in opts:
        if o in ("-h", "--help"):
            usage()
            sys.exit(0)
        elif o in ("-j", "--jobs"):
            Options.num_jobs = int(a, 10)

    all_bridge_transport_reqs = []
    with multiprocessing.Pool(Options.num_jobs) as pool:
        for bridge_transport_reqs in pool.imap_unordered(process_file, inputs):
            all_bridge_transport_reqs.append(bridge_transport_reqs)

    bridge_transport_reqs = pd.concat(all_bridge_transport_reqs)

    # Keep only the most recent "published" for each "end". We have already done
    # this (per file) in process_bridge_extra_infos. We do it again here to
    # remove overlaps in the seams between files. It would suffice to do it
    # only here, at the top level, but doing it also per file reduces memory
    # usage.
    bridge_transport_reqs = (
        pd.DataFrame(bridge_transport_reqs)
            .sort_values("published")
            .groupby(["fingerprint", "nickname", "transport", "end"])
            .last()
            .reset_index()
    )

    # Distribute over dates.
    bridge_ips_transport_bydate = {
        "date": [],
        "fingerprint": [],
        "nickname": [],
        "transport": [],
        "transport_reqs": [],
    }
    for row in bridge_transport_reqs.itertuples():
        for (date, frac_int, _) in segment_datetime_interval(row.begin, row.end):
            bridge_ips_transport_bydate["date"].append(date)
            bridge_ips_transport_bydate["fingerprint"].append(row.fingerprint)
            bridge_ips_transport_bydate["nickname"].append(row.nickname)
            bridge_ips_transport_bydate["transport"].append(row.transport)
            bridge_ips_transport_bydate["transport_reqs"].append(row.transport_reqs * frac_int)
    bridge_ips_transport_bydate = (
        pd.DataFrame(bridge_ips_transport_bydate)
            .groupby(["date", "fingerprint", "nickname", "transport"])
            .sum()
            .reset_index()
    )

    (
        bridge_ips_transport_bydate
        # Discard fingerprint and nickname and sum requests by date and
        # transport.
        [["date", "transport", "transport_reqs"]]
        .groupby(["date", "transport"]).sum().reset_index()
        # "First compute r(R) as the sum of reported successful directory
        # requests ... Estimate the number of clients per country and day using
        # the following formula:
        #   r(N) = floor(r(R) / frac / 10)"
        # We assume frac == 1.0, since we assume that the individual bridges we
        # are looking at always report directory request statistics.
        .assign(
            frac = 1.0,
            users = lambda x: x["transport_reqs"] / x["frac"] / 10,
        )
        # Turn frac into a percentage in output, for consistency with
        # metrics.torproject.org CSV files.
        .assign(frac = lambda x: x["frac"] * 100)
    ).to_csv(sys.stdout, index = False, float_format = "%.2f", columns = [
        "date",
        "transport",
        "users",
        "frac",
    ])
